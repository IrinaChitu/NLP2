{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/eu3neuom/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from utils.preprocessing import *\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_DATASET_PATH = \"./data/dataset.csv\"\n",
    "DATASET_PATH = \"./data/\"\n",
    "# run `split_dataset` only once to create train/test/val \n",
    "# split_dataset(FULL_DATASET_PATH, DATASET_PATH, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading\n",
    "train_dataframe = pd.read_csv(os.path.join(DATASET_PATH, \"train.csv\"))\n",
    "val_dataframe = pd.read_csv(os.path.join(DATASET_PATH, \"val.csv\"))\n",
    "test_dataframe = pd.read_csv(os.path.join(DATASET_PATH, \"test.csv\"))\n",
    "\n",
    "# Normalizing\n",
    "for dataframe in [train_dataframe, val_dataframe, test_dataframe]:\n",
    "    dataframe[\"lyrics\"] = [normalization(song) for song in dataframe[\"lyrics\"]]\n",
    "\n",
    "# Feature extraction\n",
    "tfidf_vect = TfidfVectorizer(min_df=5, max_df=0.8)\n",
    "tfidf_vect.fit(train_dataframe[\"lyrics\"])\n",
    "\n",
    "with open(\"./app/model/tfidf.pkl\", \"wb\") as file:\n",
    "    pickle.dump(tfidf_vect, file)\n",
    "\n",
    "def transform_data(tfidf, dataframe):\n",
    "    features = tfidf_vect.transform(dataframe[\"lyrics\"])\n",
    "    return pd.DataFrame(features.todense(), columns=tfidf.get_feature_names_out())\n",
    "\n",
    "train_features = transform_data(tfidf_vect, train_dataframe)\n",
    "val_features = transform_data(tfidf_vect, val_dataframe)\n",
    "test_features = transform_data(tfidf_vect, test_dataframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "artistToLabel = {}\n",
    "labelToArtist = {}\n",
    "for artist in train_dataframe[\"artist\"].tolist():\n",
    "    if artist not in artistToLabel.keys():\n",
    "        artistToLabel[artist] = idx\n",
    "        labelToArtist[idx] = artist\n",
    "        idx += 1\n",
    "    \n",
    "train_labels = [artistToLabel[artist] for artist in train_dataframe[\"artist\"]]\n",
    "val_labels = [artistToLabel[artist] for artist in val_dataframe[\"artist\"]]\n",
    "test_labels = [artistToLabel[artist] for artist in test_dataframe[\"artist\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4530, 6) (1510, 6) (1510, 6)\n",
      "(4529, 6) (1510, 6) (1510, 6)\n"
     ]
    }
   ],
   "source": [
    "# Remove songs with less than 20 words after normalization\n",
    "# print(train_dataframe.shape, val_dataframe.shape, test_dataframe.shape)\n",
    "# for dataframe in [train_dataframe, val_dataframe, test_dataframe]:\n",
    "#     indexes = []\n",
    "#     for idx, row in enumerate(dataframe.to_numpy()):\n",
    "#         if len(row[5]) <= 20:\n",
    "#             indexes.append(idx)\n",
    "#     dataframe.drop(index=indexes, inplace=True)\n",
    "# print(train_dataframe.shape, val_dataframe.shape, test_dataframe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Validation acc: [37.83962889330682]\n",
      "Test acc:       [38.9662027833002]\n"
     ]
    }
   ],
   "source": [
    "# 35%\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = RandomForestClassifier(n_jobs=5)\n",
    "parameters = {\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"n_estimators\": [100, 1000],\n",
    "    \"max_features\": [\"auto\", \"sqrt\"]\n",
    "}\n",
    "clf = GridSearchCV(clf, parameters, verbose=1)\n",
    "clf.fit(train_features, train_dataframe[\"artist\"])\n",
    "\n",
    "score = 0\n",
    "pred_labels = clf.predict(val_features)\n",
    "for i, name in enumerate(val_dataframe[\"artist\"]):\n",
    "    if pred_labels[i] == name:\n",
    "        score += 1\n",
    "print(f\"Validation acc: [{100.0 * score / len(pred_labels)}]\")\n",
    "\n",
    "score = 0\n",
    "pred_labels = clf.predict(test_features)\n",
    "for i, name in enumerate(test_dataframe[\"artist\"]):\n",
    "    if pred_labels[i] == name:\n",
    "        score += 1\n",
    "print(f\"Test acc:       [{100.0 * score / len(pred_labels)}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./app/model/random-forest.pkl\", \"wb\") as file:\n",
    "    pickle.dump(clf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc:       [38.9662027833002]\n"
     ]
    }
   ],
   "source": [
    "with open(\"./app/model/ranom-forwest.pkl\", \"rb\") as file:\n",
    "    loaded = pickle.load(file)\n",
    "\n",
    "    score = 0\n",
    "    pred_labels = loaded.predict(test_features)\n",
    "    for i, name in enumerate(test_dataframe[\"artist\"]):\n",
    "        if pred_labels[i] == name:\n",
    "            score += 1\n",
    "    print(f\"Test acc:       [{100.0 * score / len(pred_labels)}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Validation acc: [36.42384105960265]\n",
      "Test acc:       [33.17880794701987]\n"
     ]
    }
   ],
   "source": [
    "# 33%\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = SVC()\n",
    "parameters = {\n",
    "    \"kernel\": [\"rbf\"],\n",
    "    \"gamma\": [\"scale\", \"auto\"],\n",
    "    \"C\": [1, 10]\n",
    "}\n",
    "clf = GridSearchCV(clf, parameters, verbose=1)\n",
    "clf.fit(train_features, train_dataframe[\"artist\"])\n",
    "\n",
    "score = 0\n",
    "pred_labels = clf.predict(val_features)\n",
    "for i, name in enumerate(val_dataframe[\"artist\"]):\n",
    "    if pred_labels[i] == name:\n",
    "        score += 1\n",
    "print(f\"Validation acc: [{100.0 * score / len(pred_labels)}]\")\n",
    "\n",
    "score = 0\n",
    "pred_labels = clf.predict(test_features)\n",
    "for i, name in enumerate(test_dataframe[\"artist\"]):\n",
    "    if pred_labels[i] == name:\n",
    "        score += 1\n",
    "print(f\"Test acc:       [{100.0 * score / len(pred_labels)}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.labels[index]\n",
    "\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNN, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(5993, 4096),\n",
    "            nn.Linear(4096, 1024),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.Linear(256, 38)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "        \n",
    "trainData = MyDataset(train_features.to_numpy().astype(float), train_labels)\n",
    "validData = MyDataset(val_features.to_numpy().astype(float), val_labels)\n",
    "testData = MyDataset(test_features.to_numpy().astype(float), test_labels)\n",
    "\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(dataset = trainData, batch_size = batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(dataset = validData, batch_size = batch_size, shuffle=False)\n",
    "test_loader = DataLoader(dataset = testData, batch_size = 1, shuffle=False)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, numEpochs, criterion, optimizer, path=None):\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    best_accuracy = -1.0\n",
    "\n",
    "    for epoch in range(1, numEpochs + 1):\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        model.train()\n",
    "        for data, target in train_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data.float())\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * data.size(0)\n",
    "\n",
    "        valid_loss, validation_acc = validation(model, criterion)\n",
    "        if validation_acc > best_accuracy:\n",
    "            best_accuracy = validation_acc\n",
    "            # if path is not None:\n",
    "            #     aditionalInfo = {\"epochs\": epoch, \"acc\": best_accuracy, \"structure\": model}\n",
    "            #     saveStateDict(model.state_dict(), path, aditionalInfo)\n",
    "\n",
    "        train_loss = train_loss / len(train_loader.sampler)\n",
    "        valid_loss = valid_loss / len(valid_loader.sampler)\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        print(\n",
    "            \"Epoch: [{:2}/{:2}] \\tTraining Loss: [{:.6f}] \\tValidation Loss: {:.6f} \\tValidation acc: [{:.6f}] \\t Best acc: [{:.6f}]\".format(\n",
    "                epoch, numEpochs, train_loss, valid_loss, validation_acc, best_accuracy\n",
    "            )\n",
    "        )\n",
    "\n",
    "    \n",
    "def validation(model, criterion):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        valid_loss = 0\n",
    "        for images, labels in valid_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images.float())\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        return valid_loss, 100 * correct / total\n",
    "\n",
    "def test(model):\n",
    "    ret = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, imagePaths, _ in test_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images.float())\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            ret.append(predicted[0].item())\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 1/10] \tTraining Loss: [3.174205] \tValidation Loss: 2.841482 \tValidation acc: [21.471173] \t Best acc: [21.471173]\n",
      "Epoch: [ 2/10] \tTraining Loss: [2.512903] \tValidation Loss: 2.622900 \tValidation acc: [28.429423] \t Best acc: [28.429423]\n",
      "Epoch: [ 3/10] \tTraining Loss: [1.915205] \tValidation Loss: 2.756847 \tValidation acc: [30.682571] \t Best acc: [30.682571]\n",
      "Epoch: [ 4/10] \tTraining Loss: [1.390478] \tValidation Loss: 3.004896 \tValidation acc: [32.471836] \t Best acc: [32.471836]\n",
      "Epoch: [ 5/10] \tTraining Loss: [0.948332] \tValidation Loss: 3.538494 \tValidation acc: [32.935719] \t Best acc: [32.935719]\n",
      "Epoch: [ 6/10] \tTraining Loss: [0.611056] \tValidation Loss: 4.167316 \tValidation acc: [32.471836] \t Best acc: [32.935719]\n",
      "Epoch: [ 7/10] \tTraining Loss: [0.364365] \tValidation Loss: 4.823866 \tValidation acc: [33.134526] \t Best acc: [33.134526]\n",
      "Epoch: [ 8/10] \tTraining Loss: [0.218034] \tValidation Loss: 5.590315 \tValidation acc: [32.471836] \t Best acc: [33.134526]\n",
      "Epoch: [ 9/10] \tTraining Loss: [0.135640] \tValidation Loss: 6.094676 \tValidation acc: [32.935719] \t Best acc: [33.134526]\n",
      "Epoch: [10/10] \tTraining Loss: [0.088136] \tValidation Loss: 6.634794 \tValidation acc: [33.200795] \t Best acc: [33.200795]\n"
     ]
    }
   ],
   "source": [
    "model = DNN().to(device)\n",
    "learning_rate = 0.0001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "train(model, 10, criterion, optimizer, \"./models/CNN.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1990ea6f36b583d2d095accfa7817870dd015584f5fcdbaa3efdeb4724026fdb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
